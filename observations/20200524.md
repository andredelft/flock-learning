# Understanding the role of γ

While creating the figure for the exploration of the learning parameters, it struck me that the birds still seem to learn for γ = 0. This was worrying, because it seems to indicate that Q-learning would be unnecessary (because the future reward signal would be totally irrelevant). After some investigating, I don't think this is the case however.

I thought it might be a good idea to revisit the non-gradient reward system, to investigate the role of γ, because this allows us to understand the update function better. This is because the reward system here is such that r is only non-zero when a bird is flying eastward. In the case where followers observe a state where the majority is not flying eastward, there is no reward they can obtain. Thus the only way in which their Q-values can be positively influenced is through the future reward signal γ. Investigating this case means that we might be able to get a better grasp on the role of γ.

Thus we started four simulations:

|                 | Gradient reward |   γ  |
|:---------------:|:---------------:|:----:|
| 20200521-120133 |       Yes       |   0  |
| 20200521-120138 |       Yes       | 0.99 |
| 20200521-120143 |       No        |   0  |
| 20200521-120148 |       No        | 0.99 |

The Delta plots are shown below:

![](/data/20200521/Delta.png)

If we now calculate Delta for these, but only summing over followers and states where the majority is not flying eastward (and normalizing again), we get the following interesting plot:

![](/data/20200524/Delta_f.png)

For the gradient runs, we see a stagnation in the learning curve. This has to be caused by the gradient reward system, and the explanation might be that they cannot find a consistent strategy in this case. Remember that they are getting rewarded proportionally to the x-component of their flight direction. Thus both are going down simultaneously. Note that γ does not seem to have any influence on this however.

A similar thing should happen for the non-gradient runs however, since both Q-values should converge to zero. This does not seem to be the case however. For 120148 this might be caused by γ. But this cannot hold for 120143. The update rule for the Q-tables is in this case

![](https://render.githubusercontent.com/render/math?math=Q_t%3D(1-\alpha)Q_{t-1}%2B\alpha%20r_t)

and since the reward is always zero, both V and I should converge equally fast to zero, similar to the gradient runs. Is there some bias? I think its important to understand this graph.

Also, from this graph it seems that the red one is learning the fastest, but the learning quality is not very good, as the following plot shows:

![](/data/20200524/avg_v.png)

Here a short simulation (1500 steps) is done for each Q-table that has been saved intermediately in the exploration phases. We then calculated the average flight direction v, which we averaged for the last 1000 timesteps (so excluding initialization). We see that the motion quickly converges for all runs except for 120134. It is not clear to me why this happens.

I want to do this again, with adjustments I've done to `discrete_Vicsek`, to make sure these results are not coincedent, and reproducable.
